{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkEaPbDoq0X63V66x6/HEF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IngerMasha/week12/blob/main/Basic_of_Webscrapping_Exercises_XP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsqE6v9ZIuiU",
        "outputId": "58800e74-4e27-4e83-c0cf-e9ea22f3e894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Sports World\n",
            "\n",
            "Paragraphs:\n",
            "Your one-stop destination for the latest sports news and videos.\n",
            "Read about the latest football matches and player news.\n",
            "Watch highlights from the latest NBA games.\n",
            "Get the latest updates from the world of Grand Slam tennis.\n",
            "\n",
            "Links:\n",
            "#football\n",
            "#basketball\n",
            "#tennis\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open(\"test.html\", \"r\") as file:\n",
        "    soup = BeautifulSoup(file, \"html.parser\")\n",
        "\n",
        "title = soup.title.string\n",
        "print(\"Title:\", title)\n",
        "\n",
        "paragraphs = soup.find_all('p')\n",
        "print(\"\\nParagraphs:\")\n",
        "for para in paragraphs:\n",
        "    print(para.get_text())\n",
        "\n",
        "links = soup.find_all('a')\n",
        "print(\"\\nLinks:\")\n",
        "for link in links:\n",
        "    print(link.get('href'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "# Загрузите robots.txt\n",
        "url = \"https://en.wikipedia.org/robots.txt\"\n",
        "response = urllib.request.urlopen(url)\n",
        "robots_txt = response.read().decode('utf-8')\n",
        "\n",
        "print(robots_txt)\n"
      ],
      "metadata": {
        "id": "RFtpVXGXQjYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "response = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
        "print(\"Headers:\")\n",
        "for header in headers:\n",
        "    print(header.get_text())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YNZM7cmQmPM",
        "outputId": "dd13ad4a-5d94-462d-d4d6-76f69aac81a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headers:\n",
            "Main Page\n",
            "Welcome to Wikipedia\n",
            "From today's featured article\n",
            "Did you know ...\n",
            "In the news\n",
            "On this day\n",
            "Today's featured picture\n",
            "Other areas of Wikipedia\n",
            "Wikipedia's sister projects\n",
            "Wikipedia languages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "response = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "title = soup.title\n",
        "if title:\n",
        "    print(\"Title exists:\", title.get_text())\n",
        "else:\n",
        "    print(\"No title found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2MU0jzjQsN5",
        "outputId": "72a06ad5-c255-48aa-ebd3-0d74a620d04d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title exists: Wikipedia, the free encyclopedia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "response = requests.get(\"https://www.us-cert.gov/ncas/alerts\")\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "alerts = soup.find_all('div', class_='view-item')\n",
        "print(\"Number of alerts:\", len(alerts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "755X8cGEQus1",
        "outputId": "ee151ba5-6616-4e8e-a3ba-b6b092ac8ef7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of alerts: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "url = \"https://www.imdb.com/chart/top\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "movies = soup.select('td.titleColumn')\n",
        "num_movies = len(movies)\n",
        "print(f\"Total number of movies found: {num_movies}\")\n",
        "num_to_sample = min(10, num_movies)  # Если фильмов меньше 10, выбираем их все\n",
        "top_movies = random.sample(movies, num_to_sample)\n",
        "\n",
        "for movie in top_movies:\n",
        "    title = movie.a.text\n",
        "    year = movie.span.text.strip('()')\n",
        "    summary_td = movie.find_next('td', class_='summary')\n",
        "    summary = summary_td.text.strip() if summary_td else \"No summary available\"\n",
        "    print(f\"Title: {title}, Year: {year}, Summary: {summary}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxUWmmW8Qydl",
        "outputId": "86f95fa2-361d-46f2-d84d-a429cdefc410"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of movies found: 0\n"
          ]
        }
      ]
    }
  ]
}